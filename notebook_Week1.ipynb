{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (1.9.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sacremoses in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from sacremoses) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from sacremoses) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\emilh\\anaconda3\\envs\\aigamecollabenv\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers torch\n",
    "%pip install sentencepiece\n",
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/datasets/coastalcph/tydi_xor_rc\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics for a) is in the following blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/datasets/process\n",
    "\n",
    "def get_lang(dataset, language):\n",
    "    return dataset.filter(lambda sample : sample[\"lang\"] == language)\n",
    "\n",
    "def get_lang_length(dataset, language):\n",
    "    return len(get_lang(dataset, language))\n",
    "\n",
    "def get_answerable(dataset):\n",
    "    return dataset.filter(lambda sample : sample[\"answerable\"])\n",
    "\n",
    "def get_unanswerable(dataset):\n",
    "    return dataset.filter(lambda sample : not(sample[\"answerable\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('N_train_finnish', 2126),\n",
       " ('N_train_japan', 2301),\n",
       " ('N_train_russian', 1983),\n",
       " ('N_val_finnish', 2126),\n",
       " ('N_val_japan', 2301),\n",
       " ('N_val_russian', 1983)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get number of samples for each langugage for train and validation set\n",
    "train_finnish = get_lang(train_set, \"fi\")\n",
    "val_finnish = get_lang(train_set, \"fi\")\n",
    "\n",
    "train_russian = get_lang(train_set, \"ru\")\n",
    "val_russian = get_lang(train_set, \"ru\")\n",
    "\n",
    "train_japan = get_lang(train_set, \"ja\")\n",
    "val_japan = get_lang(train_set, \"ja\")\n",
    "\n",
    "{(\"N_train_finnish\", len(train_finnish)), (\"N_val_finnish\", len(val_finnish)), (\"N_train_japan\", len(train_japan)), (\"N_val_japan\", len(val_japan)),\n",
    " (\"N_train_russian\", len(train_russian)), (\"N_val_russian\", len(val_russian))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('N_train_finnish_ans', 1872),\n",
       " ('N_train_japan_ans', 1929),\n",
       " ('N_train_russian_ans', 1756),\n",
       " ('N_val_finnish_ans', 1872),\n",
       " ('N_val_japan_ans', 1929),\n",
       " ('N_val_russian_ans', 1756)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get number of answerables for each language:\n",
    "ans_train_finnish = len(get_answerable(train_finnish))\n",
    "ans_val_finnish = len(get_answerable(val_finnish))\n",
    "\n",
    "ans_train_russian = len(get_answerable(train_russian))\n",
    "ans_val_russian = len(get_answerable(val_russian))\n",
    "\n",
    "ans_train_japan = len(get_answerable(train_japan))\n",
    "ans_val_japan = len(get_answerable(val_japan))\n",
    "\n",
    "{(\"N_train_finnish_ans\", ans_train_finnish), (\"N_val_finnish_ans\", ans_val_finnish), (\"N_train_japan_ans\", ans_train_japan), (\"N_val_japan_ans\", ans_val_japan),\n",
    " (\"N_train_russian_ans\", ans_train_russian), (\"N_val_russian_ans\", ans_val_russian)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "['15', '13000', 'Army and Air Force of the Russian Soviet Federative Socialist Republic', 'São Paulo', 'heavy rain', 'During the Three Kingdoms era', '1870', '1971', '15', '13000', 'Army and Air Force of the Russian Soviet Federative Socialist Republic', 'São Paulo', 'heavy rain', 'in November 2004', 'During the Three Kingdoms era', 'October 21, 1957', 'July 17, 1985', 'In the province of Varsinais-Suomen. It is 12 km from Masku to Raisio, 15 km to Naantali and 18 km to Turku', 'a government area surrounded by a castle', 'more than 55 million', '15', '13000', 'Jefferson Davis', 'transition series of elements', 'São Paulo', 'heavy rain', 'November 10, 1994', 'During the Three Kingdoms era', 'October 21, 1957', '1870', '1971', 'In the province of Varsinais-Suomen. It is 12 km from Masku to Raisio, 15 km to Naantali and 18 km to Turku']\n"
     ]
    }
   ],
   "source": [
    "#Assert that answer for unanswered is Yes or No \n",
    "\n",
    "def get_invalids(dataset):\n",
    "    unanswered_dataset = get_unanswerable(dataset)\n",
    "    invalids = unanswered_dataset.filter(lambda sample : not(sample[\"answer\"] == \"no\" or sample[\"answer\"] == \"yes\"))\n",
    "    return invalids\n",
    "\n",
    "invalids = get_invalids(train_finnish)\n",
    "print(len(invalids))\n",
    "print(invalids[\"answer\"])\n",
    "\n",
    "#Some of the the questions that cannot be answered from the context has answers that is not \"yes\" or \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "True\n",
      "1772\n",
      "Lalan (Born: 1774 AD - Died: October 17, 1890 AD) was a Bengali with many talents; Who is also known as Fakir Lalan, Lalan Sai, Lalan Shah, Mahatma Lalan etc. He is a spiritual Baul saint, humanist, social reformer and philosopher. He was the lyricist, composer and singer of numerous songs. Lalan is considered one of the pioneers of Baul music and is also referred to as the 'Baul-Emperor'. It was through his songs that Baul song gained popularity in the 19th century. Lalan was a humanitarian saint. He who removed from all kinds of ethnic differences including religion, caste, gotra and gave humanity the highest place. He is from this non-sectarian attitude\n"
     ]
    }
   ],
   "source": [
    "#Assert that if answerable, then answer is a substring in context\n",
    "def verify_answerable_is_substring_in_context(dataset):\n",
    "    answered_dataset = get_answerable(dataset)\n",
    "    return answered_dataset.filter(lambda sample : not(sample[\"answer\"] in sample[\"context\"]))\n",
    "\n",
    "def verify_errors(invalids):\n",
    "    return invalids.filter(lambda sample : not(sample[\"answerable\"] == True and sample[\"answer_start\"] == -1))\n",
    "\n",
    "invalids = verify_answerable_is_substring_in_context(train_set)\n",
    "undetected_errors = verify_errors(invalids)\n",
    "print(invalids[\"answer_start\"][1])\n",
    "print(invalids[\"answerable\"][1])\n",
    "print(invalids[\"answer\"][1])\n",
    "print(invalids[\"context\"][1])\n",
    "\n",
    "#Note: Some questions are answerable but -1 in answer_start. Fix this by changing them to unanswerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_set.to_pandas()\n",
    "validation_df = validation_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['lang'].isin(['fi', 'ja', 'ru'])]\n",
    "validation_df = validation_df[validation_df['lang'].isin(['fi', 'ja', 'ru'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Cleaning\n",
    "\n",
    "def clean_dataset(df):\n",
    "    df.loc[(df[\"answerable\"].isin([True])) & (df[\"answer_start\"].isin([-1])), \"answerable\"] = False\n",
    "\n",
    "def verify_cleaned_dataset(df):\n",
    "    print(len(df[(df[\"answerable\"].isin([True])) & (df[\"answer_start\"].isin([-1]))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_of_answer_inlang(df):\n",
    "    not_null_count = df['answer_inlang'].notna().sum()\n",
    "    total_count = len(df)\n",
    "    print(f\"Number of rows where 'answer_inlang' is not null: {not_null_count} out of {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'answer_inlang' is not null: 150 out of 6410\n",
      "Number of rows where 'answer_inlang' is not null: 300 out of 1380\n"
     ]
    }
   ],
   "source": [
    "print_num_of_answer_inlang(train_df)\n",
    "print_num_of_answer_inlang(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Cells are for question b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'fi': (\n",
    "        MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-fi-en'), \n",
    "        MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fi-en')\n",
    "    ),\n",
    "    'ja': (\n",
    "        MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ja-en'), \n",
    "        MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ja-en')\n",
    "    ),\n",
    "    'ru': (\n",
    "        MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ru-en'), \n",
    "        MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translated_questions(df, language):\n",
    "    model, tokenizer = models[language]\n",
    "    filtered_df = df[df['lang'] == language]\n",
    "    translations = []\n",
    "    for question in filtered_df['question'].dropna():\n",
    "        translated_question = translate_text(question, model, tokenizer)\n",
    "        translations.append(translated_question)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(df, language):\n",
    "    questions = ' '.join(get_translated_questions(df, language))\n",
    "    words = word_tokenize(questions.lower())\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(5)\n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in questions for language 'fi':\n",
      "the: 1263\n",
      "what: 772\n",
      "was: 532\n",
      "of: 462\n",
      "in: 442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['fi', 'ja', 'ru']\n",
    "for lang in languages:\n",
    "    common_words = get_most_common_words(train_df, lang)\n",
    "    print(f\"Most common words in questions for language '{lang}':\")\n",
    "    for word, count in common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next is for Question c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m translater \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfi\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfi\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m questions_processed \u001b[38;5;241m=\u001b[39m [process_question(question, translater, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m df_train_finnish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     33\u001b[0m context_processed \u001b[38;5;241m=\u001b[39m [process_context(context) \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m df_train_finnish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(questions_processed)\n",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m translater \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfi\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfi\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m questions_processed \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslater\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m df_train_finnish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     33\u001b[0m context_processed \u001b[38;5;241m=\u001b[39m [process_context(context) \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m df_train_finnish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(questions_processed)\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mprocess_question\u001b[1;34m(question, translater, tokenizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_question\u001b[39m(question, translater, tokenizer):\n\u001b[1;32m----> 6\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslater\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(translated\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      8\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum()]\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mtranslate_text\u001b[1;34m(text, model, tokenizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     translated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      5\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m translated_text\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\transformers\\generation\\utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2060\u001b[0m     )\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2064\u001b[0m         input_ids,\n\u001b[0;32m   2065\u001b[0m         beam_scorer,\n\u001b[0;32m   2066\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2067\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   2068\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2069\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2070\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2071\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2072\u001b[0m     )\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2085\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\transformers\\generation\\utils.py:3238\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3241\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1416\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1396\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1397\u001b[0m         )\n\u001b[0;32m   1399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1400\u001b[0m     input_ids,\n\u001b[0;32m   1401\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1415\u001b[0m )\n\u001b[1;32m-> 1416\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1418\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emilh\\anaconda3\\envs\\AIGameCollabEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#TODO: Move translation to beginning of pipeline\n",
    "\n",
    "def process_question(question, translater, tokenizer):\n",
    "    translated = translate_text(question, translater, tokenizer)\n",
    "    words = word_tokenize(translated.lower())\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    return words\n",
    "\n",
    "def process_context(context):\n",
    "    words = [word for word in context if word.isalnum()]\n",
    "    return words\n",
    "\n",
    "def question_in_context_freq(question_tokens, context_tokens):\n",
    "    unique_question_tokens = set(question_tokens)\n",
    "    unique_context_tokens = set(context_tokens)\n",
    "    cnt = len(unique_question_tokens.intersection(unique_context_tokens))\n",
    "    return cnt/len(unique_question_tokens)\n",
    "\n",
    "def get_freqs(questions, contexts):\n",
    "    pairs = zip(questions, contexts)\n",
    "    freqs = []\n",
    "    for pair in pairs:\n",
    "        freqs.append(question_in_context_freq(pair[0],pair[1]))\n",
    "    return freqs\n",
    "\n",
    "df_train_finnish = train_df[train_df[\"lang\"].isin([\"fi\"])]\n",
    "translater = models[\"fi\"][0]\n",
    "tokenizer = models[\"fi\"][1]\n",
    "\n",
    "questions_processed = [process_question(question, translater, tokenizer) for question in df_train_finnish[\"question\"]]\n",
    "context_processed = [process_context(context) for context in df_train_finnish[\"context\"]]\n",
    "print(questions_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
